{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e3883b-5dc0-46d7-afde-e0f45d283038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from rich import print as rprint\n",
    "\n",
    "# Import util functions\n",
    "from helper import set_openai_key,\\\n",
    "                    test_openai_api,\\\n",
    "                    create_openai_client,\\\n",
    "                    print_pretty,\\\n",
    "                    function_to_schema,\\\n",
    "                    count_tiktoken_length\n",
    "\n",
    "# Import tools\n",
    "from helper import generate_tree_string,\\\n",
    "                    get_lines_from_file,\\\n",
    "                    search_code, \\\n",
    "                    get_object_definition\n",
    "\n",
    "print(\"Import successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7668ea18-1d63-40c1-95ab-07959eb7c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "comp_dir = \"konwinski-prize\"\n",
    "comp_kaggle_evaluation_dir = os.path.join(comp_dir, \"kaggle_evaluation\")\n",
    "comp_kprize_setup_dir = os.path.join(comp_dir, \"kprize_setup\")\n",
    "\n",
    "comp_data_zip_path = os.path.join(comp_dir, \"data.a_zip\")\n",
    "comp_data_dir = os.path.join(comp_dir, \"data\")\n",
    "comp_data_parquet_path = os.path.join(comp_data_dir, \"data.parquet\")\n",
    "comp_conda_packages_dir = os.path.join(comp_data_dir, \"conda_packages\")\n",
    "comp_pip_packages_dir = os.path.join(comp_data_dir, \"pip_packages\")\n",
    "comp_repo_configs_dir = os.path.join(comp_data_dir, \"repo_configs\")\n",
    "comp_repos_dir = os.path.join(comp_data_dir, \"repos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2623174-9556-4476-a0ae-96d371239667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "konwinski-prize/data/repos/repo__astropy__astropy-17048\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "\n",
    "kprize_df = pd.read_parquet(comp_data_parquet_path)\n",
    "\n",
    "row = kprize_df.iloc[idx]\n",
    "problem_statement = row[\"problem_statement\"]\n",
    "instance_id = row[\"instance_id\"]\n",
    "repo_path = os.path.join(comp_repos_dir, f'repo__{instance_id}')\n",
    "\n",
    "print(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2695d75-4256-457a-ab4d-ecae993b0db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1284\n",
      "QTable cannot take `dimensionless_unscaled` when creating table from `data`\n",
      "### Description\n",
      "\n",
      "Attempting to create an table with a column in units `dimensionless_unscaled` leads to a crash, because a guard against setting a column attribute to `np.ma.masked` or `None` triggers a comparison between `np.ma.masked` and Unit which eventually resolves to a `ZeroDivisionError`.\n",
      "\n",
      "### Expected behavior\n",
      "\n",
      "A `QTable` is created. \n",
      "\n",
      "### How to Reproduce\n",
      "\n",
      "```python\n",
      "from astropy.table import QTable, Column\n",
      "import numpy as np\n",
      "data = np.ones((5,2))\n",
      "tt = QTable(data=data, names=[\"a\",\"weight\"],units={\"weight\":u.dimensionless_unscaled})\n",
      "```\n",
      "Results in \n",
      "```\n",
      "---------------------------------------------------------------------------\n",
      "ZeroDivisionError                         Traceback (most recent call last)\n",
      "Cell In[15], line 2\n",
      "      1 data = np.ones((5,2))\n",
      "----> 2 tt = QTable(data=data, names=[\"a\",\"weight\"],units={\"weight\":u.dimensionless_unscaled})\n",
      "      3 tt\n",
      "\n",
      "File ~/miniconda3/envs/datapipe- \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(count_tiktoken_length(problem_statement))\n",
    "\n",
    "print(problem_statement[:1000],\"\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9093f5-197b-47e8-8b7e-f80ecf021095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key set successfully.\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "set_openai_key()\n",
    "\n",
    "test_openai_api()\n",
    "\n",
    "client = create_openai_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277bc933-d68f-4344-837a-0a916d149e8b",
   "metadata": {},
   "source": [
    "#### Run step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0995008f-f4a4-471f-b281-b450cd9eba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-4o\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "### Context\n",
    "You are a software automation agent responsible for diagnosing and resolving issues in a repository.  \n",
    "Your task is to configure and utilize the provided tools effectively.  \n",
    "\n",
    "If the available tools are insufficient, analyze the gap, propose suitable alternatives, and justify their necessity.  \n",
    "\n",
    "### Metadata\n",
    "- Repository root directory: {repo_path}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f3371fb-b5e0-4e03-ae24-ad68d53d7787",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [generate_tree_string, generate_tree_string, search_code, get_object_definition]\n",
    "tool_schemas = [function_to_schema(tool) for tool in tools]\n",
    "tools_map = {tool.__name__: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad068f15-89b8-4808-add8-42138b43d005",
   "metadata": {},
   "source": [
    "**Note:** As of now, OpenAI's GPT models have a 1024-character limit for tool function descriptions, and there isn't a GPT model that allows for a larger description length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c176adb7-6136-4e34-b68a-a152dffce4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for executing the tools returned in the model's response\n",
    "def handle_tool_calls(tool_calls, messages):\n",
    "    for tool_call in tool_calls:   \n",
    "        function = tools_map[tool_call.function.name]\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        result = function(**function_args)\n",
    "        messages.append({\"role\": \"tool\", \"content\": result, \"tool_call_id\": tool_call.id})\n",
    "        \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eadc387c-5523-4445-aa4c-3095bbaf8a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"tool_calls\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"audio\": null,\n",
      "                \"content\": null,\n",
      "                \"function_call\": null,\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"tool_calls\": [\n",
      "                    {\n",
      "                        \"function\": {\n",
      "                            \"arguments\": \"{\\\"root_directory\\\":\\\".\\\",\\\"search_string\\\":\\\"def _set_column_attribute\\\"}\",\n",
      "                            \"name\": \"search_code\"\n",
      "                        },\n",
      "                        \"id\": \"call_fL7LET5jLEukvLmv3xX9nF5D\",\n",
      "                        \"type\": \"function\"\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1740875946,\n",
      "    \"id\": \"chatcmpl-B6ReEu3IaPnoo22wOMCOJ8QjvCcXS\",\n",
      "    \"model\": \"gpt-4o-2024-08-06\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": \"default\",\n",
      "    \"system_fingerprint\": \"fp_eb9dce56a8\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 25,\n",
      "        \"completion_tokens_details\": {\n",
      "            \"accepted_prediction_tokens\": 0,\n",
      "            \"audio_tokens\": 0,\n",
      "            \"reasoning_tokens\": 0,\n",
      "            \"rejected_prediction_tokens\": 0\n",
      "        },\n",
      "        \"prompt_tokens\": 2161,\n",
      "        \"prompt_tokens_details\": {\n",
      "            \"audio_tokens\": 0,\n",
      "            \"cached_tokens\": 0\n",
      "        },\n",
      "        \"total_tokens\": 2186\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Generate a patch to resolve the following repository issue:\\n{problem_statement}\"\n",
    "\n",
    "prompt = prompt.format(problem_statement=problem_statement)\n",
    "\n",
    "messages.append({\"role\":\"user\",\"content\":prompt})\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"system\", \"content\": system_prompt}] + messages,\n",
    "            tools=tool_schemas,\n",
    "        )\n",
    "\n",
    "print_pretty(completion.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda4c37-3f23-4bd5-8ae5-4eb0ef0383d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d7eba-1fab-4064-abeb-8a43ac909943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
