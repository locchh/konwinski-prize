{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd0950f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-17T05:17:26.180100Z",
     "iopub.status.busy": "2025-01-17T05:17:26.179677Z",
     "iopub.status.idle": "2025-01-17T05:17:37.393497Z",
     "shell.execute_reply": "2025-01-17T05:17:37.392212Z"
    },
    "papermill": {
     "duration": 11.221739,
     "end_time": "2025-01-17T05:17:37.395676",
     "exception": false,
     "start_time": "2025-01-17T05:17:26.173937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import kaggle_evaluation.konwinski_prize_inference_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e4e38f",
   "metadata": {
    "papermill": {
     "duration": 0.002092,
     "end_time": "2025-01-17T05:17:37.400914",
     "exception": false,
     "start_time": "2025-01-17T05:17:37.398822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The evaluation API requires that you set up a server which will respond to inference requests. We have already defined the server; you just need write the predict function. When we evaluate your submission on the hidden test set the client defined in `konwinski_prize_gateway` will run in a different container with direct access to the hidden test set and hand off the data.\n",
    "\n",
    "Your code will always have access to the published copies of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de5a5434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T05:17:37.407407Z",
     "iopub.status.busy": "2025-01-17T05:17:37.406362Z",
     "iopub.status.idle": "2025-01-17T05:17:37.412152Z",
     "shell.execute_reply": "2025-01-17T05:17:37.410990Z"
    },
    "papermill": {
     "duration": 0.010668,
     "end_time": "2025-01-17T05:17:37.413886",
     "exception": false,
     "start_time": "2025-01-17T05:17:37.403218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_count = None\n",
    "\n",
    "def get_number_of_instances(num_instances: int) -> None:\n",
    "    \"\"\" The very first message from the gateway will be the total number of instances to be served.\n",
    "    You don't need to edit this function.\n",
    "    \"\"\"\n",
    "    global instance_count\n",
    "    instance_count = num_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc49ff32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T05:17:37.419613Z",
     "iopub.status.busy": "2025-01-17T05:17:37.419272Z",
     "iopub.status.idle": "2025-01-17T05:17:37.427451Z",
     "shell.execute_reply": "2025-01-17T05:17:37.426306Z"
    },
    "papermill": {
     "duration": 0.013025,
     "end_time": "2025-01-17T05:17:37.429286",
     "exception": false,
     "start_time": "2025-01-17T05:17:37.416261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_prediction = True\n",
    "\n",
    "\n",
    "def predict(problem_statement: str, repo_archive: io.BytesIO, pip_packages_archive: io.BytesIO, env_setup_cmds_templates: list[str]) -> str:\n",
    "    \"\"\" Replace this function with your inference code.\n",
    "    Args:\n",
    "        problem_statement: The text of the git issue.\n",
    "        repo_path: A BytesIO buffer path with a .tar containing the codebase that must be patched. The gateway will make this directory available immediately before this function runs.\n",
    "        pip_packages_archive: A BytesIO buffer path with a .tar containing the wheel files necessary for running unit tests.\n",
    "        env_setup_cmds_templates: Commands necessary for installing the pip_packages_archive.\n",
    "    \"\"\"\n",
    "    global first_prediction\n",
    "    if not first_prediction:\n",
    "        return None  # Skip issue.\n",
    "\n",
    "    # Unpack the codebase to be patched\n",
    "    with open('repo_archive.tar', 'wb') as f:\n",
    "        f.write(repo_archive.read())\n",
    "    repo_path = 'repo'\n",
    "    if os.path.exists(repo_path):\n",
    "        shutil.rmtree(repo_path)\n",
    "    shutil.unpack_archive('repo_archive.tar', extract_dir=repo_path)\n",
    "    os.remove('repo_archive.tar')\n",
    "\n",
    "    \"\"\"\n",
    "    Unpack pip_packages if you want to run unit tests on your patch.\n",
    "    Note that editing unit tests with your patch -- even to add valid tests -- can cause your submission to be flagged as a failure.\n",
    "    Most of the relevant repos use pytest for running tests. You will almost certainly need to run only a subset of the unit tests to avoid running out of inference time.\n",
    "    \"\"\"\n",
    "    with open('pip_packages_archive.tar', 'wb') as f:\n",
    "        f.write(pip_packages_archive.read())\n",
    "    pip_packages_path = '/path/to/pip_packages'\n",
    "    if os.path.exists(pip_packages_path):\n",
    "        shutil.rmtree(pip_packages_path)\n",
    "    shutil.unpack_archive('pip_packages_archive.tar', extract_dir=pip_packages_path)\n",
    "    os.remove('pip_packages_archive.tar')\n",
    "\n",
    "    # Get env setup cmds by setting the pip_packages_path\n",
    "    env_setup_cmds = [cmd.format(pip_packages_path=pip_packages_path) for cmd in env_setup_cmds_templates]\n",
    "\n",
    "    # Run env setup for the repo\n",
    "    subprocess.run(\n",
    "        \"\\n\".join(env_setup_cmds),\n",
    "        shell=True,\n",
    "        executable=\"/bin/bash\",\n",
    "        cwd=repo_path,\n",
    "    )\n",
    "\n",
    "    first_prediction = False\n",
    "    # Instead of a valid diff, let's just submit a generic string. This will definitely fail.\n",
    "    return \"Hello World\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2ddf6",
   "metadata": {
    "papermill": {
     "duration": 0.001866,
     "end_time": "2025-01-17T05:17:37.434914",
     "exception": false,
     "start_time": "2025-01-17T05:17:37.433048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When your notebook is run on the hidden test set, inference_server.serve must be called within 15 minutes of the notebook starting or the gateway will throw an error. If you need more than 15 minutes to load your model you can do so during the very first predict call, which does not have the usual 30 minute response deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae16b85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T05:17:37.440376Z",
     "iopub.status.busy": "2025-01-17T05:17:37.440021Z",
     "iopub.status.idle": "2025-01-17T05:19:53.016041Z",
     "shell.execute_reply": "2025-01-17T05:19:53.014784Z"
    },
    "papermill": {
     "duration": 135.581256,
     "end_time": "2025-01-17T05:19:53.018226",
     "exception": false,
     "start_time": "2025-01-17T05:17:37.436970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing uv installation found. Skipping uv installation.\n",
      "Installing Python 3.11...\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.konwinski_prize_inference_server.KPrizeInferenceServer(\n",
    "    get_number_of_instances,   \n",
    "    predict\n",
    ")\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        data_paths=(\n",
    "            '/kaggle/input/konwinski-prize/',  # Path to the entire competition dataset\n",
    "            '/kaggle/tmp/konwinski-prize/',   # Path to a scratch directory for unpacking data.a_zip.\n",
    "        ),\n",
    "        use_concurrency=True,  # This can safely be disabled for purposes of local testing if necessary.\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10454310,
     "sourceId": 84795,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 150.702302,
   "end_time": "2025-01-17T05:19:53.944529",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-17T05:17:23.242227",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
